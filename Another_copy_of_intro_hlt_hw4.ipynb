{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxGlwcV9P8CB",
        "outputId": "05bde5c3-8bf4-40d6-bfb4-1e3974120a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-19 02:26:24--  https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw4-files/student/required_files.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw4-files/student/required_files.zip [following]\n",
            "--2023-12-19 02:26:25--  https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/assignments/hw4-files/student/required_files.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2395 (2.3K) [application/zip]\n",
            "Saving to: ‘required_files.zip’\n",
            "\n",
            "required_files.zip  100%[===================>]   2.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-12-19 02:26:25 (34.4 MB/s) - ‘required_files.zip’ saved [2395/2395]\n",
            "\n",
            "Archive:  required_files.zip\n",
            "  inflating: requirements.txt        \n",
            "   creating: tests/\n",
            "  inflating: tests/model-generate-test.py  \n",
            "  inflating: vocabulary.py           \n"
          ]
        }
      ],
      "source": [
        "# Downloads required packages and files\n",
        "required_files = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw4-files/student/required_files.zip\"\n",
        "! wget $required_files && unzip -o required_files.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The required.txt file needs to be modified with 2 things:\n",
        "1) sklearn needs to be replaced to be scikit-learn\n",
        "2) torchmetrics should be replaced by torchmetrics == 0.11.4"
      ],
      "metadata": {
        "id": "j8uTBuijFZZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXg7EviU_avk",
        "outputId": "d71bc8a2-7bd3-4639-e9ef-04b8d1457e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datascience in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.17.6)\n",
            "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (6.1.12)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (5.5.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (7.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (6.5.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (5.9.2)\n",
            "Collecting dill (from -r requirements.txt (line 12))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.23.5)\n",
            "Collecting otter-grader==4.0.1 (from -r requirements.txt (line 14))\n",
            "  Downloading otter_grader-4.0.1-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.9/166.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfkit (from -r requirements.txt (line 16))\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting PyPDF2 (from -r requirements.txt (line 17))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wkhtmltopdf (from -r requirements.txt (line 18))\n",
            "  Downloading wkhtmltopdf-0.2.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides===6.2.0 (from -r requirements.txt (line 21))\n",
            "  Downloading overrides-6.2.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.1.0+cu121)\n",
            "Collecting pytorch-lightning~=1.7.7 (from -r requirements.txt (line 25))\n",
            "  Downloading pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.1/708.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (1.2.2)\n",
            "Collecting transformers~=4.23.1 (from -r requirements.txt (line 30))\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==0.11.4 (from -r requirements.txt (line 31))\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onedrivedownloader (from -r requirements.txt (line 32))\n",
            "  Downloading onedrivedownloader-1.1.3-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (6.0.1)\n",
            "Collecting python-on-whales (from otter-grader==4.0.1->-r requirements.txt (line 14))\n",
            "  Downloading python_on_whales-0.68.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (2.31.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (1.14.1)\n",
            "Collecting jupytext (from otter-grader==4.0.1->-r requirements.txt (line 14))\n",
            "  Downloading jupytext-1.16.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (8.1.7)\n",
            "Collecting fica>=0.2.0 (from otter-grader==4.0.1->-r requirements.txt (line 14))\n",
            "  Downloading fica-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (2.84.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (3.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from otter-grader==4.0.1->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.11.4->-r requirements.txt (line 31)) (23.2)\n",
            "Requirement already satisfied: folium>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from datascience->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from datascience->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from datascience->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from datascience->-r requirements.txt (line 1)) (5.15.0)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from datascience->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter_client->-r requirements.txt (line 2)) (6.3.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->-r requirements.txt (line 9)) (2.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->-r requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->-r requirements.txt (line 11)) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->-r requirements.txt (line 11)) (4.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (2.1.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (4.66.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (2.15.1)\n",
            "Collecting pyDeprecate>=0.3.1 (from pytorch-lightning~=1.7.7->-r requirements.txt (line 25))\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 28)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 28)) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.23.1->-r requirements.txt (line 30)) (0.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.23.1->-r requirements.txt (line 30)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers~=4.23.1->-r requirements.txt (line 30))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.18.1)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (5.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch->-r requirements.txt (line 22)) (3.9.1)\n",
            "Collecting jedi>=0.16 (from ipython->datascience->-r requirements.txt (line 1))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (3.0.43)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 11)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 11)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 11)) (0.13.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter_client->-r requirements.txt (line 2)) (4.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (2.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (3.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 14)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 14)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 14)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader==4.0.1->-r requirements.txt (line 14)) (2023.11.17)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (6.5.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->-r requirements.txt (line 10)) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->-r requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 14)) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 14)) (4.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader==4.0.1->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.10.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->datascience->-r requirements.txt (line 1)) (8.2.3)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.9 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.10.13)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 22)) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 22)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 22)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 22)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 22)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->-r requirements.txt (line 22)) (4.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (4.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->datascience->-r requirements.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=1.0.0->jupytext->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->datascience->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->datascience->-r requirements.txt (line 1)) (0.2.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->otter-grader==4.0.1->-r requirements.txt (line 14)) (3.2.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.0.6)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.2.0->otter-grader==4.0.1->-r requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (0.2.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning~=1.7.7->-r requirements.txt (line 25)) (0.5.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 6)) (2.21)\n",
            "Building wheels for collected packages: wkhtmltopdf\n",
            "  Building wheel for wkhtmltopdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wkhtmltopdf: filename=wkhtmltopdf-0.2-py3-none-any.whl size=11134 sha256=bb4c70a017038167d6484ac46633bd6c94171bc5f7566cf323e8d4050231dd20\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/07/ce/f83cde6e9b0a616486c17e1e172965d90c6cdb85ae4c9f77fb\n",
            "Successfully built wkhtmltopdf\n",
            "Installing collected packages: wkhtmltopdf, tokenizers, pdfkit, PyPDF2, pyDeprecate, overrides, jedi, dill, python-on-whales, onedrivedownloader, transformers, torchmetrics, pytorch-lightning, jupytext, fica, otter-grader\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed PyPDF2-3.0.1 dill-0.3.7 fica-0.3.1 jedi-0.19.1 jupytext-1.16.0 onedrivedownloader-1.1.3 otter-grader-4.0.1 overrides-6.2.0 pdfkit-1.0.0 pyDeprecate-0.3.2 python-on-whales-0.68.0 pytorch-lightning-1.7.7 tokenizers-0.13.3 torchmetrics-0.11.4 transformers-4.23.1 wkhtmltopdf-0.2\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6jrenPcP8CD"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "\n",
        "grader = otter.Notebook(colab=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ikcyU1Y5P8CG"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Paste your Colab notebook link here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3jwaRI-yk1"
      },
      "source": [
        "First let's set up some helper code and import the libraries which we will use in this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JEfVMyWP8CI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Tuple, Any, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "\n",
        "try:\n",
        "    from vocabulary import Vocabulary, encode_as_tensor, decode_as_str\n",
        "except:\n",
        "    from files.vocabulary import Vocabulary, encode_as_tensor, decode_as_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRqxwAMXP8CI",
        "outputId": "d897451e-0358-49b4-8a7a-5a7b68f02b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The notebook is running for \"student\".\n",
            "Students should make sure you are running under the \"student\" mode.\n",
            "You are using \"gpu\".\n"
          ]
        }
      ],
      "source": [
        "# Checks whether it is in the autograder grading mode\n",
        "# Checks whether GPU accelerators are available\n",
        "is_autograder = os.path.exists('is_autograder.py')\n",
        "if torch.cuda.is_available() and not is_autograder:\n",
        "    accelerator = 'gpu'\n",
        "elif os.environ.get('COLAB_TPU_ADDR') is not None and not is_autograder:\n",
        "    accelerator = 'tpu'\n",
        "else:\n",
        "    accelerator = 'cpu'\n",
        "print(f'The notebook is running for \"{\"autograder\" if is_autograder else \"student\"}\".')\n",
        "print('Students should make sure you are running under the \"student\" mode.')\n",
        "print(f'You are using \"{accelerator}\".')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXnFTWF0P8CJ",
        "outputId": "ae8ccc5a-6d55-4222-8914-e7b9422c7199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "777"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Seed everything to make sure all experiments are reproducible\n",
        "pl.seed_everything(seed=777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUpUyMrGP8CJ"
      },
      "outputs": [],
      "source": [
        "# Defines constants\n",
        "DATA_URL = \"https://github.com/jhu-intro-hlt/jhu-intro-hlt.github.io/raw/master/assignments/hw4-files/student/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrfYVOQLP8CK",
        "outputId": "f6811bad-88d6-4317-bd13-4544d2764c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Attention!!!\n",
        "# Set this to True, if you are using local machine instead of Colab\n",
        "RUN_LOCALLY = False\n",
        "# But you have to make sure that your machine has CUDA support\n",
        "# Otherwise the training would be super slow.\n",
        "LOCAL_DATA_STORE_PATH = 'librispeech_data'  # If run locally, please configure a path to store the dataset.\n",
        "# Only try to use the Colab and Google Drive when not under the autograder environment\n",
        "if not is_autograder:\n",
        "    if not RUN_LOCALLY:\n",
        "        try:\n",
        "            LIBRISPEECH_DATA_PATH = './gdrive/My Drive/librispeech_data'\n",
        "            from google.colab import drive\n",
        "\n",
        "            drive.mount('/content/gdrive')\n",
        "        except:  # Fall back to local storage\n",
        "            LIBRISPEECH_DATA_PATH = LOCAL_DATA_STORE_PATH\n",
        "    else:\n",
        "        LIBRISPEECH_DATA_PATH = LOCAL_DATA_STORE_PATH\n",
        "else:\n",
        "    LIBRISPEECH_DATA_PATH = LOCAL_DATA_STORE_PATH\n",
        "os.makedirs(LIBRISPEECH_DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPii9YVKP8CK"
      },
      "outputs": [],
      "source": [
        "if not is_autograder:\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(LIBRISPEECH_DATA_PATH, url='train-clean-100', download=True)\n",
        "    dev_dataset = torchaudio.datasets.LIBRISPEECH(LIBRISPEECH_DATA_PATH, url='dev-clean', download=True)\n",
        "else:\n",
        "    train_dataset = None\n",
        "    dev_dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lz6zxYMdP8CL"
      },
      "source": [
        "# Vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiq5u_ufP8CL"
      },
      "outputs": [],
      "source": [
        "char_to_map = char_map_str = \"\"\"'\n",
        "a\n",
        "b\n",
        "c\n",
        "d\n",
        "e\n",
        "f\n",
        "g\n",
        "h\n",
        "i\n",
        "j\n",
        "k\n",
        "l\n",
        "m\n",
        "n\n",
        "o\n",
        "p\n",
        "q\n",
        "r\n",
        "s\n",
        "t\n",
        "u\n",
        "v\n",
        "w\n",
        "x\n",
        "y\n",
        "z\"\"\"\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for c in char_to_map.split('\\n'):\n",
        "    vocab.add_token(token=c.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgvWfaVk-HNk"
      },
      "source": [
        "## Downloading the data\n",
        "We will use a 100h subset of LibrSpeech to train our models. Run the code cell below to download the training and testing data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Feature extraction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8sr9jnP8CO"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f1cF5-GioPc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "     torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64),\n",
        "     torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "     torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "# Feature extraction for test data. Use the same sample rate and number of mels\n",
        "# as the training data. No SpecAugment is needed here.\n",
        "\n",
        "valid_audio_transforms =  torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzyP-xEtP8CP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ASRBatch:\n",
        "    spectrograms: torch.Tensor  # (batch_size, channel, feature, time)\n",
        "    labels: torch.Tensor  # (batch_size, label_len, num_labels)\n",
        "    attention_mask: torch.Tensor  # (batch_size, time)\n",
        "    label_mask: torch.Tensor  # (batch_size, label_len)\n",
        "    label_strs: List[str]\n",
        "\n",
        "\n",
        "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
        "    \"\"\"Wraps PyTorch dataset as a lightning data module.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            datasets: Dict[str, Dataset],\n",
        "            vocab: Vocabulary,\n",
        "            batch_size: int = 32,\n",
        "            shuffle: bool = True\n",
        "    ):\n",
        "        super(SpeechRecognitionDataModule, self).__init__()\n",
        "\n",
        "        self.datasets: Dict[str, Dataset] = {k: v for k, v in datasets.items() if v is not None}\n",
        "        self.vocab = vocab\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad_sequence(seq: torch.Tensor, max_length: int, padding_value: Union[int, float, bool]) -> torch.Tensor:\n",
        "        seq_len = seq.shape[-1]\n",
        "        if seq_len < max_length:\n",
        "            return torch.cat(\n",
        "                [seq, torch.tensor([padding_value] * (max_length - seq_len), dtype=seq.dtype, device=seq.device)],\n",
        "                dim=-1\n",
        "            )\n",
        "        else:\n",
        "            return seq\n",
        "\n",
        "    def collate_fn(self, data, phase='train'):\n",
        "        spectrograms = []\n",
        "        labels = []\n",
        "        label_strs = []\n",
        "        input_lengths = []\n",
        "        label_lengths = []\n",
        "        for (waveform, _, utterance, _, _, _) in data:\n",
        "            if phase == 'train':\n",
        "                spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "            elif phase == 'val' or phase == 'test':\n",
        "                spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "            else:\n",
        "                raise ValueError\n",
        "            spectrograms.append(spec)\n",
        "            label_str = utterance.lower()\n",
        "            label = encode_as_tensor(self.vocab, label_str).squeeze(0)\n",
        "            labels.append(label)\n",
        "            label_strs.append(label_str)\n",
        "            input_lengths.append(spec.shape[0] // 2)\n",
        "            label_lengths.append(label.shape[-1])\n",
        "\n",
        "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.vocab.pad_id())\n",
        "\n",
        "        max_input_len = max(input_lengths)\n",
        "        max_label_len = max(label_lengths)\n",
        "        attention_mask = torch.stack(\n",
        "            [\n",
        "                self._pad_sequence(torch.ones(l, dtype=torch.bool), max_length=max_input_len, padding_value=False)\n",
        "                for l in input_lengths\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "        label_mask = torch.stack(\n",
        "            [\n",
        "                self._pad_sequence(torch.ones(l, dtype=torch.bool), max_length=max_label_len, padding_value=False)\n",
        "                for l in label_lengths\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        return ASRBatch(\n",
        "            spectrograms=spectrograms,\n",
        "            labels=labels,\n",
        "            attention_mask=attention_mask,\n",
        "            label_mask=label_mask,\n",
        "            label_strs=label_strs\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.datasets['train'],\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=self.shuffle,\n",
        "                          collate_fn=lambda x: self.collate_fn(x, phase='train'))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.datasets['val'],\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=lambda x: self.collate_fn(x, phase='val'))\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.datasets['test'],\n",
        "                          batch_size=self.batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=lambda x: self.collate_fn(x, phase='test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1gCspkXP8CQ"
      },
      "outputs": [],
      "source": [
        "test_datamodule = SpeechRecognitionDataModule(\n",
        "    datasets={\n",
        "        'train': train_dataset,\n",
        "        'val': dev_dataset\n",
        "    },\n",
        "    vocab=vocab,\n",
        "    batch_size=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG7LCteuP8CQ",
        "outputId": "4feb8920-a597-4a86-a273-3c47c3274a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASRBatch(spectrograms=tensor([[[[2.8718e-03, 5.1259e-03, 5.1706e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [3.1303e-03, 6.7686e-03, 5.8033e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [3.6288e-03, 1.0869e-02, 7.1558e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [9.9442e-04, 1.0929e-03, 2.1029e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [6.6392e-04, 9.4669e-04, 1.6397e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [5.3676e-04, 4.2598e-04, 5.0813e-04,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[1.2488e-01, 1.6741e-02, 4.3402e-01,  ..., 1.3716e+00,\n",
            "           1.3200e+00, 5.5934e-01],\n",
            "          [1.0877e-01, 1.3796e-02, 3.4552e-01,  ..., 1.0400e+00,\n",
            "           9.9259e-01, 4.2907e-01],\n",
            "          [5.6131e-02, 4.6071e-03, 7.4245e-02,  ..., 4.1466e-02,\n",
            "           9.1925e-03, 3.5386e-02],\n",
            "          ...,\n",
            "          [5.5554e-04, 5.0497e-04, 1.3583e-03,  ..., 4.8830e-05,\n",
            "           1.7413e-05, 1.2499e-05],\n",
            "          [1.9096e-03, 1.1830e-03, 1.1194e-03,  ..., 1.2778e-05,\n",
            "           1.4074e-05, 1.0325e-05],\n",
            "          [1.4214e-03, 1.8027e-03, 9.9082e-04,  ..., 2.2310e-05,\n",
            "           5.3491e-06, 1.0715e-05]]],\n",
            "\n",
            "\n",
            "        [[[6.4609e-06, 2.2477e-06, 5.5620e-07,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [7.2540e-06, 1.9101e-06, 2.3280e-06,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [8.9508e-06, 8.3324e-07, 7.1030e-06,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [1.0380e-05, 7.7997e-06, 3.3724e-06,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0858e-05, 5.9074e-06, 8.5085e-06,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [4.7514e-06, 6.9460e-06, 4.9514e-06,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[3.6248e-04, 9.5446e-03, 2.8309e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [4.6681e-04, 1.1271e-02, 2.9607e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [7.2457e-04, 1.5284e-02, 3.1124e-03,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [2.3261e-05, 6.2198e-05, 3.9838e-05,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [2.0923e-05, 2.3011e-05, 1.9546e-05,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.0462e-05, 1.9922e-05, 1.3476e-05,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]]]]), labels=tensor([[ 0, 25, 13, 20, 24, 10,  4,  7,  6,  8, 16,  4,  9, 20, 20, 23, 28,  6,\n",
            "         30, 24,  4,  6, 23, 10,  4, 27, 10, 23, 30,  4, 13,  6, 19,  9, 30,  4,\n",
            "         14, 19,  4, 25, 14, 18, 10,  4, 20, 11,  4,  9,  6, 19, 12, 10, 23,  4,\n",
            "          9, 20,  4, 30, 20, 26,  4,  6, 17, 28,  6, 30, 24,  4, 13,  6, 27, 10,\n",
            "          4, 25, 13, 23, 10, 10,  4,  9, 20, 20, 23, 28,  6, 30, 24,  4,  6, 24,\n",
            "         16, 10,  9,  4, 13,  6, 21, 21, 30,  4, 15,  6,  8, 16,  1,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
            "        [ 0,  9,  6, 23, 25, 14, 19, 12,  4, 11, 26, 23, 14, 20, 26, 24,  4, 12,\n",
            "         17,  6, 19,  8, 10, 24,  4,  7, 10, 13, 14, 19,  9,  4, 25, 13, 10, 18,\n",
            "          4, 20, 19, 10,  4, 25, 13, 14, 19, 12,  4, 14,  4, 20,  7, 24, 10, 23,\n",
            "         27, 10,  9,  4, 28, 13, 14,  8, 13,  4, 17, 20, 20, 16, 10,  9,  4, 28,\n",
            "         10, 17, 17,  4, 11, 20, 23,  4, 26, 24,  4, 25, 13, 10, 30,  4,  6, 17,\n",
            "         17,  4, 12, 20, 25,  4, 20, 26, 25,  4, 26, 21, 20, 19,  4, 25, 13, 10,\n",
            "          4, 20, 21, 21, 20, 24, 14, 25, 10,  4, 24, 14,  9, 10,  4, 11, 23, 20,\n",
            "         18,  4, 24, 14, 17, 27, 10, 23,  4, 28, 10, 17, 17,  4, 25, 13, 10, 23,\n",
            "         10,  4, 28, 10,  4, 24, 25, 20, 20,  9,  4, 25, 28, 20,  4, 20, 19,  4,\n",
            "         20, 19, 10,  4, 24, 14,  9, 10,  4, 11, 14, 27, 10,  4, 20, 19,  4, 25,\n",
            "         13, 10,  4, 20, 25, 13, 10, 23,  4, 25, 13, 10,  4, 21, 14, 25,  4,  7,\n",
            "         10, 25, 28, 10, 10, 19,  4, 26, 24,  1],\n",
            "        [ 0, 13, 10,  4, 10, 27, 10, 19,  4, 11, 10, 17, 25,  4, 25, 13,  6, 25,\n",
            "          4, 28, 14, 25, 13, 20, 26, 25,  4, 25, 13, 14, 24,  4, 18, 20, 24, 22,\n",
            "         26, 14, 25, 20,  4, 11, 14, 17, 17, 10,  9,  4,  6, 25, 18, 20, 24, 21,\n",
            "         13, 10, 23, 10,  4,  6, 23, 20, 26, 19,  9,  4, 13, 14, 18,  4,  6, 19,\n",
            "          9,  4, 25, 13,  6, 25,  4, 18, 20, 24, 22, 26, 14, 25, 20,  4, 21,  6,\n",
            "         24, 25, 10,  4, 18, 14, 19, 12, 17, 10,  9,  4, 28, 14, 25, 13,  4, 21,\n",
            "         10, 23, 24, 21, 14, 23,  6, 25, 14, 20, 19,  4, 28, 13, 14,  8, 13,  4,\n",
            "         13, 14, 24,  4, 13,  6, 19,  9,  4, 24, 18, 10,  6, 23, 10,  9,  4, 20,\n",
            "         27, 10, 23,  4, 13, 14, 24,  4, 11,  6,  8, 10,  4,  6, 19,  9,  4, 25,\n",
            "         13,  6, 25,  4, 26, 19,  8, 10,  6, 24, 14, 19, 12,  4, 14, 23, 23, 14,\n",
            "         25,  6, 25, 14, 20, 19,  4,  6, 17, 17,  4, 20, 27, 10, 23,  4, 13, 14,\n",
            "         24,  4,  7, 20,  9, 30,  1,  3,  3,  3],\n",
            "        [ 0, 20, 11,  4, 28, 13,  6, 25,  4,  8, 20, 19, 24, 10, 22, 26, 10, 19,\n",
            "          8, 10,  4, 14, 24,  4, 14, 25,  4, 25, 20,  4, 26, 24,  4, 25, 13,  6,\n",
            "         25,  4, 25, 13, 10,  4, 14, 19,  9, 14,  6, 19,  4, 23,  6,  8, 10,  4,\n",
            "         28,  6, 24,  4,  9, 14, 27, 14,  9, 10,  9,  4, 14, 19, 25, 20,  4, 11,\n",
            "         20, 26, 23,  4,  8, 17,  6, 24, 24, 10, 24,  4, 25, 13,  6, 25,  4, 20,\n",
            "         19,  4, 25, 13, 10,  4,  7,  6, 19, 16, 24,  4, 20, 11,  4, 25, 13, 10,\n",
            "          4, 19, 14, 17, 10,  4,  6, 19,  9,  4, 25, 13, 10,  4, 12,  6, 19, 12,\n",
            "         10, 24,  4,  7, 17, 20, 20,  9,  4,  6, 19,  9,  4, 21, 20, 24, 14, 25,\n",
            "         14, 20, 19,  4, 11, 20, 23, 18, 10, 23, 17, 30,  4,  9, 10, 25, 10, 23,\n",
            "         18, 14, 19, 10,  9,  1,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3]]), attention_mask=tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ...,  True,  True,  True],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False]]), label_mask=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False]]), label_strs=['those back doorways are very handy in time of danger do you always have three doorways asked happy jack', 'darting furious glances behind them one thing i observed which looked well for us they all got out upon the opposite side from silver well there we stood two on one side five on the other the pit between us', 'he even felt that without this mosquito filled atmosphere around him and that mosquito paste mingled with perspiration which his hand smeared over his face and that unceasing irritation all over his body', 'of what consequence is it to us that the indian race was divided into four classes that on the banks of the nile and the ganges blood and position formerly determined'])\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = test_datamodule.train_dataloader()\n",
        "for i, bc in enumerate(train_dataloader):\n",
        "    if i > 0:\n",
        "        break\n",
        "    print(bc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "from torchmetrics import CharErrorRate, WordErrorRate\n"
      ],
      "metadata": {
        "id": "khAYcUPLCEgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8bXE2s1P8CR"
      },
      "outputs": [],
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by the model architecture in DeepSpeech 2.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = nn.functional.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = nn.functional.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    \"\"\"Bidirectional GRU with layer normalization and dropout.\"\"\"\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = nn.functional.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Extracts features for ASR using CNNs and Residual connections.\"\"\"\n",
        "\n",
        "    def __init__(self, n_feats, n_cnn_layers, out_channels=32, dropout=0.1):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.initial_cnn = nn.Conv2d(1, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.residual_cnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(out_channels, out_channels, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_cnn(x)\n",
        "        x = self.residual_cnn_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "UBGUnQ9v3cYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \"\"\"Speech Recognition Model combining feature extraction, bidirectional GRUs, and a classifier.\"\"\"\n",
        "\n",
        "    def __init__(self, n_feats, n_class, rnn_dim, n_rnn_layers, n_cnn_layers, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor(n_feats=n_feats//2, n_cnn_layers=n_cnn_layers, out_channels=32, dropout=dropout)\n",
        "\n",
        "        self.fully_connected = nn.Linear(n_feats * 16, rnn_dim)  # Adjust the multiplication factor based on the final output size\n",
        "        self.rnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2, hidden_size=rnn_dim, dropout=dropout, batch_first=True)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim * 2, rnn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Reshape for RNN layers\n",
        "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.rnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "F3RqH9hGAMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrapped_generate(model_to_wrap: nn.Module, **kwargs) -> torch.Tensor:\n",
        "    \"\"\"Generates output sequences using greedy decoding.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_to_wrap : nn.Module\n",
        "        The ASR model to be used for generating the output.\n",
        "    kwargs\n",
        "        Arguments to be passed to the model for generating output.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The generated output sequences in the form of torch.Tensor.\n",
        "    \"\"\"\n",
        "    input_tensor = kwargs.get('input_tensor')\n",
        "    blank_label = kwargs.get('blank_label', 28)  # Default blank label\n",
        "\n",
        "    model_to_wrap.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model_to_wrap(input_tensor)  # Obtain logits from the model\n",
        "        logits = logits.transpose(0, 1)  # Transpose to (time, batch, n_class)\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "        best_paths = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "    decoded_sequences = []\n",
        "    max_length = 0\n",
        "    for path in best_paths:\n",
        "        sequence = []\n",
        "        previous = None\n",
        "        for label in path:\n",
        "            if label != previous and label != blank_label:\n",
        "                sequence.append(label.item())\n",
        "            previous = label\n",
        "        max_length = max(max_length, len(sequence))\n",
        "        decoded_sequences.append(sequence)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    padded_sequences = [seq + [blank_label]*(max_length - len(seq)) for seq in decoded_sequences]\n",
        "\n",
        "    return torch.tensor(padded_sequences)\n"
      ],
      "metadata": {
        "id": "AYFRaH_lAPO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import CharErrorRate, WordErrorRate\n",
        "from torch.nn import CTCLoss\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "\n",
        "\n",
        "\n",
        "class AutomaticSpeechRecognitionTask(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, vocab: Vocabulary, learning_rate: float = 0.001):\n",
        "        super(AutomaticSpeechRecognitionTask, self).__init__()\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.cer = CharErrorRate()\n",
        "        self.wer = WordErrorRate()\n",
        "        self.criterion = CTCLoss(blank=self.vocab.pad_id())\n",
        "\n",
        "    def training_step(self, batch: ASRBatch, batch_idx: int) -> STEP_OUTPUT:\n",
        "        spectrograms = batch.spectrograms\n",
        "        labels = batch.labels\n",
        "        input_lengths = batch.attention_mask.sum(dim=-1)\n",
        "        label_lengths = batch.label_mask.sum(dim=-1)\n",
        "\n",
        "        spectrograms, labels = spectrograms.to(self.device), labels.to(self.device)\n",
        "\n",
        "        logits = self.model(spectrograms).transpose(0, 1)  # (time, batch, n_class)\n",
        "        loss = self.criterion(logits, labels, input_lengths, label_lengths)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: ASRBatch, batch_idx: int):\n",
        "        spectrograms = batch.spectrograms\n",
        "        labels = batch.labels\n",
        "        input_lengths = batch.attention_mask.sum(dim=-1)\n",
        "        label_lengths = batch.label_mask.sum(dim=-1)\n",
        "\n",
        "        spectrograms, labels = spectrograms.to(self.device), labels.to(self.device)\n",
        "\n",
        "        logits = self.model(spectrograms).transpose(0, 1)  # (time, batch, n_class)\n",
        "        loss = self.criterion(logits, labels, input_lengths, label_lengths)\n",
        "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        # Decode each sequence in the batch using wrapped_generate\n",
        "        decoded_sequences = [wrapped_generate(model_to_wrap=self.model, input_tensor=spec.unsqueeze(0), blank_label=self.vocab.pad_id())[0] for spec in spectrograms]\n",
        "\n",
        "        # Convert decoded sequences to strings\n",
        "        decoded_strs = [''.join([self.vocab.idx2token(idx) for idx in sequence]) for sequence in decoded_sequences]\n",
        "\n",
        "        # Convert label sequences to strings using idx2token\n",
        "        label_strs = [''.join([self.vocab.idx2token(idx) for idx in label_seq.tolist()]) for label_seq in labels]\n",
        "\n",
        "        # Calculate and log CER and WER\n",
        "        curr_cer = self.cer(decoded_strs, label_strs)\n",
        "        curr_wer = self.wer(decoded_strs, label_strs)\n",
        "        self.log('val_cer', curr_cer, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_wer', curr_wer, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
      ],
      "metadata": {
        "id": "s00WEns4BRYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import logging\n",
        "from pytorch_lightning.profiler import SimpleProfiler, AdvancedProfiler\n",
        "\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)\n",
        "profiler = AdvancedProfiler()\n",
        "\n",
        "# Configure data module\n",
        "asr_datamodule = SpeechRecognitionDataModule(\n",
        "    datasets={\n",
        "        'train': train_dataset,\n",
        "        'val': dev_dataset\n",
        "    },\n",
        "    vocab=vocab,\n",
        "    batch_size=4  # Assuming the batch size used in training\n",
        ")\n",
        "\n",
        "asr_model = SpeechRecognitionModel(\n",
        "    n_feats= 64,  # Assuming 128-dimensional MelSpectrogram features\n",
        "    n_class=len(vocab),  # Number of classes based on the vocabulary size\n",
        "    rnn_dim=512,  # Assuming the RNN dimension used in training\n",
        "    n_rnn_layers=5,  # Assuming the number of RNN layers used in training\n",
        "    n_cnn_layers=3,  # Assuming the number of CNN layers used in training\n",
        "    dropout=0.1  # Dropout rate\n",
        ")\n",
        "\n",
        "# Check if the code is running in autograder mode or not\n",
        "if is_autograder:\n",
        "    # Assumes a checkpoint URL is provided for autograder mode\n",
        "    CHECKPOINT_TO_DOWNLOAD = ...\n",
        "    from onedrivedownloader import download as onedrive_download\n",
        "    onedrive_download(CHECKPOINT_TO_DOWNLOAD, filename='checkpoint.ckpt', unzip=False)\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask.load_from_checkpoint(\n",
        "        checkpoint_path='checkpoint.ckpt',\n",
        "        vocab=vocab,\n",
        "        model=asr_model\n",
        "    )\n",
        "else:\n",
        "    # Initialize the ASR task for training\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask(\n",
        "        vocab=vocab,\n",
        "        model=asr_model,\n",
        "        learning_rate=5e-4  # Assuming the learning rate used in training\n",
        "    )\n",
        "\n",
        "    # Configure the trainer\n",
        "    asr_trainer = pl.Trainer(\n",
        "        accelerator=accelerator,\n",
        "        min_epochs=10,  # Minimum number of epochs\n",
        "        max_epochs=20,  # Maximum number of epochs\n",
        "        profiler=profiler,\n",
        "        log_every_n_steps=1,\n",
        "        callbacks=[pl.callbacks.EarlyStopping(monitor='val_cer', mode='min')]\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    asr_trainer.fit(model=asr_pl_module, datamodule=asr_datamodule)\n",
        "    print(profiler.summary())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "6ca6e40a6cb142498d3bdf9aeccd39d5",
            "28fe06295008428b8d8be090bdac22e0",
            "55260efcff2445d3ae051ca32cae81a1",
            "102c683b0dbb493a8bdcb98dea09b3be",
            "c5db2e9ecdab493aa7bd7429004f5bb5",
            "5488e0989d3841c0bb1ff7edee075c62",
            "07ad8607dbef4dcd9a0a9418c76413ec",
            "3f22629832d4405ba3be2bb94cac4e10",
            "2cd666f303b44000a7f03f1ec2590791",
            "be2946ea84e34837b0ab1abc97d452a6",
            "996527665a41411883f57ee30341be76",
            "a26373115af942f0b46696488b4cdead",
            "e870907881404e74b18684a3147b4285",
            "38cd6b4d42af49539a68ec377d1d5a88",
            "7eb3cc85f1b8473b85d13cd825e15a0e",
            "975bcc6dea4f43be9814c10d49345cd1",
            "75f8ea4787b94959a0acc78eaf529f55",
            "bfdf496df3e8400a9ea649b14e688b8c",
            "2e782a858c974030a918852d7d129cb1",
            "62bdc5f2977441729317a55339fc2c79",
            "895341478649452eac11e1c800b6d0a2",
            "a5d784ca6ba44da9951198a7f1f51fe1"
          ]
        },
        "id": "EqSaNmj7BwXm",
        "outputId": "d838d967-f556-4f91-bbcd-d361aa7774a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type                   | Params\n",
            "-----------------------------------------------------\n",
            "0 | model     | SpeechRecognitionModel | 23.2 M\n",
            "1 | cer       | CharErrorRate          | 0     \n",
            "2 | wer       | WordErrorRate          | 0     \n",
            "3 | criterion | CTCLoss                | 0     \n",
            "-----------------------------------------------------\n",
            "23.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "23.2 M    Total params\n",
            "92.729    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ca6e40a6cb142498d3bdf9aeccd39d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:98: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a26373115af942f0b46696488b4cdead"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below (the next 4 classes, Feautre Extractor, Speech recognition, wrapped generate and automatic speech recognition task) have all been modified. The other thing thats been modified is the data initialization in the training part."
      ],
      "metadata": {
        "id": "Rl7buVUUFq0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-ICC2QiQGaB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, n_feats, n_features, output_size=64):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # Convolutional layer\n",
        "        self.conv = nn.Conv2d(1, n_features, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Calculate the size after the convolution\n",
        "        conv_height = n_feats // 2  # This is an assumption; adjust based on your actual input size\n",
        "\n",
        "        # Linear layer to get the output to the desired size\n",
        "        self.linear = nn.Linear(n_features * conv_height, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Flatten and pass through the linear layer\n",
        "        x = x.permute(0, 2, 3, 1)  # Rearrange to (batch_size, height, width, channels)\n",
        "        x = x.view(x.size(0), -1)  # Flatten height and channels\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, vocab, n_feats, n_features, rnn_dim, n_rnn_layers, dropout):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.feature_extractor = FeatureExtractor(n_feats, n_features, output_size=n_feats)\n",
        "\n",
        "        # Bidirectional GRU\n",
        "        self.rnn = nn.GRU(input_size=n_feats, hidden_size=rnn_dim, num_layers=n_rnn_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Linear layer to map RNN outputs to token predictions\n",
        "        self.lm_head = nn.Linear(rnn_dim * 2, len(vocab))  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None, labels=None, label_mask=None):\n",
        "        # Process input through feature extractor and RNN\n",
        "        x = self.feature_extractor(input_values)\n",
        "        x, _ = self.rnn(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Compute CTC loss if labels are provided\n",
        "        loss = None\n",
        "        if labels is not None and label_mask is not None:\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "            input_lengths = torch.full((x.size(0),), x.size(1), dtype=torch.long)\n",
        "            target_lengths = label_mask.sum(-1)\n",
        "\n",
        "            flattened_targets = labels.masked_select(label_mask)\n",
        "            loss = nn.functional.ctc_loss(\n",
        "                log_probs=log_probs,\n",
        "                targets=flattened_targets,\n",
        "                input_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=self.vocab.pad_id(),\n",
        "                zero_infinity=True  # To handle potential infinities in the loss\n",
        "            )\n",
        "\n",
        "        return CausalLMOutput(loss=loss, logits=logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnGiEwsLSacI"
      },
      "outputs": [],
      "source": [
        "def wrapped_generate(model_to_wrap: nn.Module, input_values: torch.Tensor, vocab: Vocabulary) -> List[str]:\n",
        "    \"\"\"\n",
        "    Uses the model to generate predictions and decodes them into strings using\n",
        "    the provided Vocabulary's decoding function.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_to_wrap : nn.Module\n",
        "        The ASR model to generate predictions.\n",
        "    input_values : torch.Tensor\n",
        "        The input tensor containing spectrogram features.\n",
        "    vocab : Vocabulary\n",
        "        The Vocabulary object for decoding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        The list of decoded strings.\n",
        "    \"\"\"\n",
        "    model_to_wrap.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model\n",
        "        output = model_to_wrap(input_values=input_values)\n",
        "        logits = output.logits\n",
        "\n",
        "        # Greedy decoding: choose the most probable token at each time step\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Use the Vocabulary's decoding function\n",
        "        decoded_outputs = decode_as_str(vocab, predicted_ids)\n",
        "\n",
        "    return decoded_outputs\n",
        "\n",
        "# Example usage:\n",
        "# decoded_strings = wrapped_generate(asr_model, input_spectrogram_tensor, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTzAOL6QSjG1"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import LightningModule\n",
        "from torchmetrics import CharErrorRate, WordErrorRate\n",
        "\n",
        "class AutomaticSpeechRecognitionTask(LightningModule):\n",
        "    def __init__(self, model: nn.Module, vocab: Vocabulary, learning_rate: float = 0.001):\n",
        "        super(AutomaticSpeechRecognitionTask, self).__init__()\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # Metrics\n",
        "        self.cer = CharErrorRate()\n",
        "        self.wer = WordErrorRate()\n",
        "\n",
        "    def forward(self,\n",
        "                input_values: torch.Tensor,\n",
        "                attention_mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None,\n",
        "                label_mask: Optional[torch.Tensor] = None):\n",
        "        return self.model(input_values, attention_mask, labels, label_mask)\n",
        "\n",
        "    def training_step(self, batch: ASRBatch, batch_idx: int):\n",
        "        # Access data from batch using attributes\n",
        "        input_values = batch.spectrograms\n",
        "        labels = batch.labels\n",
        "        attention_mask = batch.attention_mask\n",
        "        label_mask = batch.label_mask\n",
        "        # label_strs = batch.label_strs  # Not needed for training_step\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self(input_values, attention_mask, labels, label_mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Logging\n",
        "        batch_size = input_values.size(0)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: ASRBatch, batch_idx: int):\n",
        "        input_values = batch.spectrograms\n",
        "        labels = batch.labels\n",
        "        attention_mask = batch.attention_mask\n",
        "        label_mask = batch.label_mask\n",
        "        label_strs = batch.label_strs\n",
        "\n",
        "        outputs = self(input_values, attention_mask, labels, label_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Greedy decoding and metrics calculation\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        decoded_outputs = decode_as_str(self.vocab, predicted_ids)\n",
        "        curr_cer = self.cer(decoded_outputs, label_strs)\n",
        "        curr_wer = self.wer(decoded_outputs, label_strs)\n",
        "\n",
        "        # Logging\n",
        "        batch_size = input_values.size(0)\n",
        "        self.log('val_cer', curr_cer, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
        "        self.log('val_wer', curr_wer, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un2bkAzPThCU"
      },
      "outputs": [],
      "source": [
        "# Configure data module\n",
        "asr_datamodule = SpeechRecognitionDataModule(\n",
        "    datasets={\n",
        "        'train': train_dataset,  # Your training dataset\n",
        "        'val': dev_dataset       # Your validation dataset\n",
        "    },\n",
        "    vocab=vocab,\n",
        "    batch_size=64  # Batch size, adjust as needed\n",
        ")\n",
        "\n",
        "# Parameters for the model\n",
        "n_feats = 64  # Number of Mel-spectrogram features\n",
        "n_features = 64  # Adjusted to match the output of the FeatureExtractor\n",
        "rnn_dim = 512  # Hidden size of RNN layers\n",
        "n_rnn_layers = 3  # Number of RNN layers\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Instantiate the ASR model\n",
        "asr_model = SpeechRecognitionModel(\n",
        "    vocab=vocab,\n",
        "    n_feats=n_feats,\n",
        "    n_features=n_features,\n",
        "    rnn_dim=rnn_dim,\n",
        "    n_rnn_layers=n_rnn_layers,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# Create the ASR task and train the model\n",
        "if is_autograder:\n",
        "    # Download and load the checkpoint for the autograder\n",
        "    CHECKPOINT_TO_DOWNLOAD = ...  # Replace with your checkpoint URL\n",
        "    onedrive_download(CHECKPOINT_TO_DOWNLOAD, filename='checkpoint.ckpt', unzip=False)\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask.load_from_checkpoint(\n",
        "        checkpoint_path='checkpoint.ckpt',\n",
        "        vocab=vocab,\n",
        "        model=asr_model\n",
        "    )\n",
        "else:\n",
        "    # Create a new task for training in student mode\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask(\n",
        "        model=asr_model,\n",
        "        vocab=vocab,\n",
        "        learning_rate=5e-4\n",
        "    )\n",
        "\n",
        "    # Configure the PyTorch Lightning Trainer\n",
        "    asr_trainer = pl.Trainer(\n",
        "        accelerator=accelerator,\n",
        "        min_epochs=10,\n",
        "        max_epochs=20,\n",
        "        callbacks=[pl.callbacks.EarlyStopping(monitor='val_cer', mode='min')]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    asr_trainer.fit(asr_pl_module, asr_datamodule)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvqlWJKN3cEA"
      },
      "source": [
        "Another old imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhYprhfOohEZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, n_mels: int, n_features: int):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv = nn.Conv2d(1, n_features, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.bn = nn.BatchNorm2d(n_features)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x.transpose(1, -1)  # (batch_size, time, mapped_feature)\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, vocab: Vocabulary, n_feats: int, rnn_dim: int, n_rnn_layers: int):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.feature_extractor = FeatureExtractor(n_mels=n_feats, n_features=n_feats)\n",
        "\n",
        "        # Store rnn_dim and n_rnn_layers as instance attributes\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.n_rnn_layers = n_rnn_layers\n",
        "\n",
        "        # Placeholder for the LSTM input size and the LSTM layer\n",
        "        self.rnn = nn.LSTM(input_size=2048, hidden_size=rnn_dim, num_layers=n_rnn_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_dim, len(vocab))\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None, labels=None, label_mask=None):\n",
        "\n",
        "        input_values = input_values.cuda()\n",
        "        attention_mask = attention_mask.cuda() if attention_mask is not None else None\n",
        "        labels = labels.cuda() if labels is not None else None\n",
        "        label_mask = label_mask.cuda() if label_mask is not None else None\n",
        "        features = self.feature_extractor(input_values)\n",
        "        batch_size, time, channels, feat_dim = features.size()\n",
        "\n",
        "\n",
        "\n",
        "        features = features.reshape(batch_size, time, channels * feat_dim)\n",
        "        rnn_out, _ = self.rnn(features)\n",
        "        logits = self.fc(rnn_out)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "            input_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.long)\n",
        "            target_lengths = label_mask.sum(-1)\n",
        "            flattened_targets = labels.masked_select(label_mask)\n",
        "            loss = nn.functional.ctc_loss(log_probs, flattened_targets, input_lengths, target_lengths, blank=self.vocab.pad_id(), zero_infinity=True)\n",
        "\n",
        "        return CausalLMOutput(loss=loss, logits=logits)\n",
        "\n",
        "# Assuming Vocabulary and other necessary classes are defined elsewhere\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKTAErilop5A"
      },
      "outputs": [],
      "source": [
        "def wrapped_generate(model_to_wrap: nn.Module, input_values: torch.Tensor, vocab: Vocabulary) -> List[str]:\n",
        "    model_to_wrap.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model_to_wrap(input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    decoded_outputs = [decode_as_str(vocab, ids) for ids in predicted_ids]\n",
        "    return decoded_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saz0qZr4o-wo"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import CharErrorRate, WordErrorRate\n",
        "\n",
        "class AutomaticSpeechRecognitionTask(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, vocab: Vocabulary, learning_rate: float = 0.001):\n",
        "        super(AutomaticSpeechRecognitionTask, self).__init__()\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.cer = CharErrorRate()\n",
        "        self.wer = WordErrorRate()\n",
        "\n",
        "    def training_step(self, batch: ASRBatch):\n",
        "      print(f\"Model device: {next(self.model.parameters()).device}\")\n",
        "      print(f\"Input device: {batch.spectrograms.device}\")\n",
        "\n",
        "      batch.spectrograms = batch.spectrograms.cuda()\n",
        "      batch.attention_mask = batch.attention_mask.cuda()\n",
        "      batch.labels = batch.labels.cuda()\n",
        "      batch.label_mask = batch.label_mask.cuda()\n",
        "      outputs = self.model(\n",
        "          input_values=batch.spectrograms,\n",
        "          attention_mask=batch.attention_mask,\n",
        "          labels=batch.labels,\n",
        "          label_mask=batch.label_mask\n",
        "      )\n",
        "      self.log('train_loss', outputs.loss)\n",
        "      return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch: ASRBatch, batch_idx: int):\n",
        "        print(f\"Model device: {next(self.model.parameters()).device}\")\n",
        "        print(f\"Input device: {batch.spectrograms.device}\")\n",
        "        batch.spectrograms = batch.spectrograms.cuda()\n",
        "        batch.attention_mask = batch.attention_mask.cuda()\n",
        "        outputs = self.model(\n",
        "            input_values=batch.spectrograms,\n",
        "            attention_mask=batch.attention_mask,\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        decoded_outputs = [decode_as_str(self.vocab, ids) for ids in predicted_ids]\n",
        "        curr_cer = self.cer(decoded_outputs, batch.label_strs)\n",
        "        curr_wer = self.wer(decoded_outputs, batch.label_strs)\n",
        "        self.log('val_cer', curr_cer, on_step=False, on_epoch=True)\n",
        "        self.log('val_wer', curr_wer, on_step=False, on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFyZ8LOQpAcw"
      },
      "outputs": [],
      "source": [
        "asr_datamodule = SpeechRecognitionDataModule(\n",
        "    datasets={\n",
        "        'train': train_dataset,\n",
        "        'val': dev_dataset\n",
        "    },\n",
        "    vocab=vocab,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "asr_model = SpeechRecognitionModel(\n",
        "    vocab=vocab,\n",
        "    n_feats=64,  # Number of Mel-frequency bands\n",
        "    rnn_dim=368,  # Dimensionality of the RNN\n",
        "    n_rnn_layers=3  # Number of RNN layers\n",
        ")\n",
        "\n",
        "# Check if running in autograder mode\n",
        "if is_autograder:\n",
        "    # Download the checkpoint and load it\n",
        "    # Code for downloading and loading the checkpoint\n",
        "    pass\n",
        "else:\n",
        "    # If not in autograder mode, train the model\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask(\n",
        "        model=asr_model,\n",
        "        vocab=vocab,\n",
        "        learning_rate=5e-4\n",
        "    )\n",
        "    asr_trainer = pl.Trainer(\n",
        "        accelerator=accelerator,\n",
        "        min_epochs=10,\n",
        "        max_epochs=20,\n",
        "        callbacks=[pl.callbacks.EarlyStopping(monitor='val_cer', mode='min')]\n",
        "    )\n",
        "    asr_trainer.fit(model=asr_pl_module, datamodule=asr_datamodule)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU20y64NqtRY"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics==0.11.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJSHMQkZpBnO"
      },
      "outputs": [],
      "source": [
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLTxlEIpCE6"
      },
      "source": [
        "This is the old imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXeSMw3YP8CR"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Extracts features for ASR.\n",
        "\n",
        "    If needed, you have to use your auxiliary modules to extract features.\n",
        "    Please modify the `__init__` to make it compatible with your model designs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            numFeatures\n",
        "    ):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv3 = nn.Conv2d(64, numFeatures, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Your code here\n",
        "        ...\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"This forward method processes the input spectrogram features and returns\n",
        "        a tensor that represents transformed features.\n",
        "\n",
        "        Please pay attention to the output tensor shape, you have to use `transpose`\n",
        "        to make sure dimensions aligned.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input spectrograms, in the shape of (batch_size, channel, feature, time)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            (batch_size, time, mapped_feature)\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        x = x.transpose(1, 2).transpose(2, 3).squeeze(1)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "        ...\n",
        "\n",
        "\n",
        "class SpeechRecgonitionModel(nn.Module):\n",
        "    def __init__(self, vocab, numFeatures, dimRnn, noLayers, dropout):\n",
        "        super(SpeechRecgonitionModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.featureExtractor = FeatureExtractor(numFeatures)\n",
        "        self.rnn = nn.LSTM(input_size=numFeatures, hidden_size=dimRnn, num_layers=noLayers, dropout=dropout, batch_first=True)\n",
        "        self.fullConn = nn.Linear(dimRnn, len(vocab))\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None, labels=None, label_mask=None):\n",
        "        features = self.featureExtractor(input_values)\n",
        "        print(\"Feature Shape:\", features.shape)\n",
        "\n",
        "        # Add this line to check the shape before the LSTM\n",
        "        rnn_input = features.unsqueeze(1)\n",
        "        print(\"RNN Input Shape:\", rnn_input.shape)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None and label_mask is not None:\n",
        "            target_lengths = label_mask.sum(-1)\n",
        "            flattened_targets = labels.masked_select(label_mask)\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=-1).transpose(0, 1)\n",
        "\n",
        "            # Please make sure this aligns with the number of conv kernel\n",
        "            if attention_mask is not None:\n",
        "                input_lengths = attention_mask.sum(-1)\n",
        "            else:\n",
        "                input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=logits.device)\n",
        "\n",
        "            loss = nn.functional.ctc_loss(\n",
        "                log_probs=log_probs,\n",
        "                targets=flattened_targets,\n",
        "                input_lengths=input_lengths,\n",
        "                target_lengths=target_lengths,\n",
        "                blank=self.vocab.pad_id()\n",
        "            )\n",
        "\n",
        "        return CausalLMOutput(loss=loss, logits=logits)\n",
        "\n",
        "        # Please make sure that you are returning a `CausalLMOutput`.\n",
        "        # Your code here\n",
        "        ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIQeswaLyOCs"
      },
      "outputs": [],
      "source": [
        "def GreedyDecoder(output, labels, labelLengths, blankLabel, textTransform, collapseRepeated=True):\n",
        "    argMaxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(argMaxes):\n",
        "        decode = []\n",
        "        targets.append(textTransform.int_to_text(labels[i][:labelLengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blankLabel:\n",
        "                if collapseRepeated and j != 0 and index == args[j - 1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(textTransform.int_to_text(decode))\n",
        "    return decodes, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11enXFZ4P8CS"
      },
      "outputs": [],
      "source": [
        "def wrapped_generate(model_to_wrap: nn.Module, **kwargs) -> torch.Tensor:\n",
        "    \"\"\"Wraps the generate method. This function is what will be actually\n",
        "    called by the evaluation routine for the leaderboard.\n",
        "\n",
        "    In this function, you can wrap your `generate()` method to allow\n",
        "    different generation configurations to be used at the test time.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_to_wrap : nn.Module\n",
        "        Your ASR model.\n",
        "    kwargs\n",
        "        Argument dict that passes all arguments to the generate function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Generated phoneme sequences in the form of torch.Tensor.\n",
        "        In the shape of (batch_size, time).\n",
        "    \"\"\"\n",
        "    # Please implement your decoding strategy here.\n",
        "    # You have to pass inputs to the model and run the model once (in no_grad mode)\n",
        "    # and decode with your decoding strategy to get the tensor of token ids\n",
        "    # The simplest one is the greedy decoding - recall that we did it in hw2\n",
        "    # It is also the one used in the `validation_step`.\n",
        "    # You might see multiple repeated characters. In this case, you could also implement\n",
        "    # some heuristics in this function to do post-processing to remove\n",
        "    # repeated nonsensical characters.\n",
        "    model_to_wrap.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputValues = kwargs.get('input_values')\n",
        "        labels = kwargs.get('labels')\n",
        "        labelLengths = kwargs.get('label_lengths')\n",
        "        textTransform = kwargs.get('text_transform')\n",
        "        blankLabel = kwargs.get('blank_label', 28)\n",
        "        if inputValues is None:\n",
        "            raise ValueError(\"Input values are required for generation\")\n",
        "\n",
        "        outputs = model_to_wrap(inputValues)\n",
        "\n",
        "        # Assuming the model outputs logits directly\n",
        "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "\n",
        "        # Apply Greedy Decoding\n",
        "        decodes, _ = GreedyDecoder(\n",
        "            logits,\n",
        "            labels,\n",
        "            labelLengths,\n",
        "            blankLabel,\n",
        "            textTransform,\n",
        "            collapse_repeated=True\n",
        "        )\n",
        "\n",
        "        # Convert decoded sequences to tensor\n",
        "        decodedSeq = [torch.tensor(textTransform.text_to_int(text)) for text in decodes]\n",
        "        decodedSeqPad = torch.nn.utils.rnn.pad_sequence(decodedSeq, batch_first=True)\n",
        "\n",
        "    return decodedSeqPad\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V3461puP8CS"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import CharErrorRate, WordErrorRate\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "\n",
        "\n",
        "class AutomaticSpeechRecognitionTask(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 vocab: Vocabulary,\n",
        "                 learning_rate: float = 0.001):\n",
        "        super(AutomaticSpeechRecognitionTask, self).__init__()\n",
        "        self.model = model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.cer = CharErrorRate()\n",
        "        self.wer = WordErrorRate()\n",
        "\n",
        "    def training_step(self, batch: ASRBatch) -> STEP_OUTPUT:\n",
        "        \"\"\"Defines the training step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : ASRBatch\n",
        "            The batched training instances.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            The loss computed from the ASR model.\n",
        "        \"\"\"\n",
        "        # Please make necessary modifications to accommodate your design\n",
        "        outputs = self.model(\n",
        "            input_values=batch.spectrograms,\n",
        "            attention_mask=batch.attention_mask,\n",
        "            labels=batch.labels,\n",
        "            label_mask=batch.label_mask\n",
        "        )\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, batch: ASRBatch, batch_idx: int):\n",
        "        \"\"\"Defines the validation step - for this module, we have the same\n",
        "        training and validation behaviors. Usually, we would compute a metric that is\n",
        "        used to select the best performing model checkpoint.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : ASRBatch\n",
        "            The batched training instances.\n",
        "        batch_idx: int\n",
        "            The index of the batch.\n",
        "        \"\"\"\n",
        "        # You are free to modify this function to ensure they are being called correctly.\n",
        "        outputs = self.model(\n",
        "            input_values=batch.spectrograms,\n",
        "            attention_mask=batch.attention_mask,\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "        # take argmax and decode - Using greedy decoding\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        decoded_outputs = decode_as_str(self.vocab, predicted_ids)\n",
        "        curr_cer = self.cer(decoded_outputs, batch.label_strs)\n",
        "        curr_wer = self.wer(decoded_outputs, batch.label_strs)\n",
        "        self.log('val_cer', self.cer)\n",
        "        self.log('val_wer', self.wer)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configures optimizers for the training.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROyJicS-P8CT"
      },
      "outputs": [],
      "source": [
        "# Configure data module\n",
        "asr_datamodule = SpeechRecognitionDataModule(\n",
        "    datasets={\n",
        "        'train': train_dataset,\n",
        "        'val': dev_dataset\n",
        "    },\n",
        "    vocab=vocab,\n",
        "    batch_size=64\n",
        ")\n",
        "# Please modify the model and trainer configurations to accommodate your design\n",
        "asr_model = SpeechRecgonitionModel(\n",
        "    vocab=vocab,\n",
        "    # The following args might not align with your model!\n",
        "    numFeatures=128,\n",
        "    dimRnn=512,\n",
        "    noLayers=3,\n",
        "    dropout=0.1\n",
        ")\n",
        "if is_autograder:  # You have to make sure that you checkpoint can be correctly loaded by the autograder\n",
        "    # Please upload your checkpoint to your OneDrive\n",
        "    # and put the share link below `CHECKPOINT_TO_DOWNLOAD`\n",
        "    CHECKPOINT_TO_DOWNLOAD = ...\n",
        "    # Downloads the checkpoint\n",
        "    from onedrivedownloader import download as onedrive_download\n",
        "    onedrive_download(CHECKPOINT_TO_DOWNLOAD, filename='checkpoint.ckpt', unzip=False)\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask.load_from_checkpoint(\n",
        "        checkpoint_path='checkpoint.ckpt',\n",
        "        vocab=vocab,\n",
        "        model=asr_model\n",
        "    )\n",
        "else:\n",
        "    # In the student mode, a new model would be trained\n",
        "    # You are allowed to change training hyperparameters\n",
        "    # But you are not allowed to create a new task\n",
        "    asr_pl_module = AutomaticSpeechRecognitionTask(\n",
        "        vocab=vocab,\n",
        "        model=asr_model,\n",
        "        learning_rate=5e-4\n",
        "    )\n",
        "    asr_trainer = pl.Trainer(\n",
        "        accelerator=accelerator,\n",
        "        min_epochs=10,\n",
        "        max_epochs=20,\n",
        "        callbacks=[pl.callbacks.EarlyStopping(monitor='val_cer', mode='min')]\n",
        "    )\n",
        "    asr_trainer.fit(model=asr_pl_module, datamodule=asr_datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep9BvcnV7VLv"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSUOfBNsP8CT"
      },
      "outputs": [],
      "source": [
        "asr_trainer.logged_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrv-YTIP8CT"
      },
      "outputs": [],
      "source": [
        "if not is_autograder:\n",
        "    # You can run this cell to test whether you have get your model trained\n",
        "    # You are expecting to see a phoneme sequence\n",
        "    train_dataloader = asr_datamodule.train_dataloader()\n",
        "    test_example = None\n",
        "    for i, bc in enumerate(train_dataloader):\n",
        "        if i > 0:\n",
        "            break\n",
        "        test_example = bc\n",
        "\n",
        "    print(decode_as_str(\n",
        "        asr_pl_module.model.vocab,\n",
        "        wrapped_generate(\n",
        "            model_to_wrap=asr_pl_module.model,\n",
        "            input_values=test_example.spectrograms.to(asr_pl_module.device),\n",
        "            attention_mask=test_example.attention_mask.to(asr_pl_module.device)\n",
        "        )\n",
        "    ))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ca6e40a6cb142498d3bdf9aeccd39d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28fe06295008428b8d8be090bdac22e0",
              "IPY_MODEL_55260efcff2445d3ae051ca32cae81a1",
              "IPY_MODEL_102c683b0dbb493a8bdcb98dea09b3be"
            ],
            "layout": "IPY_MODEL_c5db2e9ecdab493aa7bd7429004f5bb5"
          }
        },
        "28fe06295008428b8d8be090bdac22e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5488e0989d3841c0bb1ff7edee075c62",
            "placeholder": "​",
            "style": "IPY_MODEL_07ad8607dbef4dcd9a0a9418c76413ec",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "55260efcff2445d3ae051ca32cae81a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f22629832d4405ba3be2bb94cac4e10",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cd666f303b44000a7f03f1ec2590791",
            "value": 2
          }
        },
        "102c683b0dbb493a8bdcb98dea09b3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be2946ea84e34837b0ab1abc97d452a6",
            "placeholder": "​",
            "style": "IPY_MODEL_996527665a41411883f57ee30341be76",
            "value": " 2/2 [00:03&lt;00:00,  1.81s/it]"
          }
        },
        "c5db2e9ecdab493aa7bd7429004f5bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "5488e0989d3841c0bb1ff7edee075c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ad8607dbef4dcd9a0a9418c76413ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f22629832d4405ba3be2bb94cac4e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd666f303b44000a7f03f1ec2590791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be2946ea84e34837b0ab1abc97d452a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996527665a41411883f57ee30341be76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a26373115af942f0b46696488b4cdead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e870907881404e74b18684a3147b4285",
              "IPY_MODEL_38cd6b4d42af49539a68ec377d1d5a88",
              "IPY_MODEL_7eb3cc85f1b8473b85d13cd825e15a0e"
            ],
            "layout": "IPY_MODEL_975bcc6dea4f43be9814c10d49345cd1"
          }
        },
        "e870907881404e74b18684a3147b4285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75f8ea4787b94959a0acc78eaf529f55",
            "placeholder": "​",
            "style": "IPY_MODEL_bfdf496df3e8400a9ea649b14e688b8c",
            "value": "Epoch 0:   7%"
          }
        },
        "38cd6b4d42af49539a68ec377d1d5a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e782a858c974030a918852d7d129cb1",
            "max": 2773,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62bdc5f2977441729317a55339fc2c79",
            "value": 200
          }
        },
        "7eb3cc85f1b8473b85d13cd825e15a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895341478649452eac11e1c800b6d0a2",
            "placeholder": "​",
            "style": "IPY_MODEL_a5d784ca6ba44da9951198a7f1f51fe1",
            "value": " 200/2773 [04:44&lt;1:01:06,  1.42s/it, loss=2.93, v_num=0, train_loss_step=2.670]"
          }
        },
        "975bcc6dea4f43be9814c10d49345cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "75f8ea4787b94959a0acc78eaf529f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfdf496df3e8400a9ea649b14e688b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e782a858c974030a918852d7d129cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62bdc5f2977441729317a55339fc2c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895341478649452eac11e1c800b6d0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5d784ca6ba44da9951198a7f1f51fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}